{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03050374-58d4-4388-a334-d06a395bfd40",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - HuggingFace Text2Text Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5383e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b27d1-cd77-46d7-88f0-68748156703d",
   "metadata": {},
   "source": [
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "In this demo notebook, we use the SageMaker Python SDK to **fine-tune a Text2Text model**. Such a model takes prompting text as input and generates text as output. The prompt can include a task description in natural language. Accordingly, the model can be used for a variety of NLP tasks (e.g., text summarization, question answering, etc.).\n",
    "\n",
    "We will fine-tune a pre-trained **FLAN T5 model** from [Hugging Face](https://huggingface.co/docs/transformers/model_doc/flan-t5). While pre-trained FLAN T5 models can be used \"as is\" for many tasks, fine-tuning can improve model performance on a particular task or language domain. As an example, we will fine-tune the model for a task that was not used for pre-training. After fine-tuning we will deploy two inference endpoints, one with a pre-trained and one with a fine-tuned model. We will then run the same inference query against both endpoints and compare results.\n",
    "\n",
    "*This notebook was tested on ml.t3.medium Amazon SageMaker Notebook instance with conda_python3 kernel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1706e56-3d74-4f2c-b5cd-695acca57d5c",
   "metadata": {},
   "source": [
    "#### In this notebook:\n",
    "1. [Setting up](#1.-Setting-up)\n",
    "1. [Fine-tuning a model](#2.-Fine-tuning-a-model)\n",
    "1. [Deploying inference endpoints](#3.-Deploying-inference-endpoints)\n",
    "1. [Running inference queries](#4.-Running-inference-queries)\n",
    "1. [Cleaning up resources](#5.-Cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30630a-685b-4be1-9c32-e50252a77b87",
   "metadata": {},
   "source": [
    "### 1. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97ab4c-f05c-4696-8040-f7b0a17ba21e",
   "metadata": {},
   "source": [
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9bf59d-f7fb-47e1-b2cf-7be6a819be96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --quiet\n",
    "!pip install sagemaker==2.166.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb228e68-2fb9-4d64-accd-dd0605774b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "tested_version = \"3.10.\"\n",
    "\n",
    "version = python_version()\n",
    "print(f\"You are using Python {version}\")\n",
    "\n",
    "if not version.startswith(tested_version):\n",
    "    print(f\"This notebook was tested with {tested_version}\")\n",
    "    print(\"Some parts might behave unexpectedly with a different Python version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a91ac-431d-4adc-8285-4e1dd73721d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use the following variables throughout the notebook. In particular, we select FLAN T5 model size and select training and inference instance types. We also obtain execution role associated with the current notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83140fe-f5d2-49ee-a433-70109cf23bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1maws_region:\u001b[0m us-east-1\n",
      "\u001b[1maws_role:\u001b[0m arn:aws:iam::327265443405:role/sagemaker-studio-domain-SageMakerExecutionRole-1F0J8QRF0545E\n",
      "\u001b[1moutput_bucket:\u001b[0m sagemaker-us-east-1-327265443405\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4ebe92-5646-4a8d-ad54-04a1c2f2dee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a smaller model as a default choice for testing purposes.\n",
    "# Larger models can produce better results, but need longer training times\n",
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ec3986-9905-4c40-b81e-ca54f78311cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7bd0e930b4cd0a996df5188ddc596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='FLAN T5 models available for fine-tuning:', index=3, layout=Layout(width='max-content'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from sagemaker.jumpstart.filters import And\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Identify FLAN T5 models that support fine-tuning\n",
    "filter_value = And(\"task == text2text\", \"framework == huggingface\", \"training_supported == true\")\n",
    "model_list = [m for m in list_jumpstart_models(filter=filter_value) if \"flan-t5\" in m]\n",
    "\n",
    "# Display the model IDs in a dropdown, for user to select\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_id,\n",
    "    description=\"FLAN T5 models available for fine-tuning:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd87d5a-d491-42f4-b0df-b19e96b16d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huggingface-text2text-flan-t5-base',\n",
       " 'huggingface-text2text-flan-t5-large',\n",
       " 'huggingface-text2text-flan-t5-small',\n",
       " 'huggingface-text2text-flan-t5-xl',\n",
       " 'huggingface-text2text-flan-t5-xxl',\n",
       " 'huggingface-text2text-flan-t5-xxl-fp16']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee3c10d0-f9e3-4463-9a6c-b8ad9f294262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-text2text-flan-t5-small\", \"*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "905ddba9-de01-4059-af82-3c178ecbed60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mmodel_id:\u001b[0m huggingface-text2text-flan-t5-small\n",
      "\u001b[1mtraining_instance_type:\u001b[0m ['ml.p3.8xlarge', 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.g5.24xlarge', 'ml.g5.48xlarge']\n",
      "\u001b[1minference_instance_type:\u001b[0m ['ml.g5.2xlarge', 'ml.g5.xlarge', 'ml.p2.xlarge', 'ml.g4dn.xlarge', 'ml.p3.2xlarge']\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.instance_types import retrieve\n",
    "\n",
    "# Instance types for training and inference\n",
    "training_instance_type = retrieve(\n",
    "    model_id=model_id, model_version=model_version, scope=\"training\"\n",
    ")\n",
    "inference_instance_type = retrieve(\n",
    "    model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{bold}model_id:{unbold} {model_id}\")\n",
    "print(f\"{bold}training_instance_type:{unbold} {training_instance_type}\")\n",
    "print(f\"{bold}inference_instance_type:{unbold} {inference_instance_type}\")\n",
    "\n",
    "training_instance_type = \"ml.g5.24xlarge\"\n",
    "inference_instance_type= \"ml.p2.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c7754-ffc3-40a0-9f85-bda3b25161d0",
   "metadata": {},
   "source": [
    "### 2. Fine-tuning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898c437-0b3a-47b2-9b25-827d824f83ec",
   "metadata": {},
   "source": [
    "FLAN T5 models were pre-trained on a variety of tasks. In this demo, we fine-tune a model for a new task. In this task, given a piece of text, the model is asked to generate questions that are relevant to the text, but cannot be answered based on provided information. Examples are given in the inference section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c377423-c7d5-4087-a175-717227d47936",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1. Preparing training data\n",
    "We will use a subset of SQuAD2.0 for supervised fine-tuning. This dataset contains questions posed by human annotators on a set of Wikipedia articles. In addition to questions with answers, SQuAD2.0 contains about 50k unanswerable questions. Such questions are plausible, but cannot be directly answered from the articles' content. We only use unanswerable questions for our task.\n",
    "\n",
    "*Citation: @article{rajpurkar2018know, title={Know what you don't know: Unanswerable questions for SQuAD},\n",
    "author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy}, journal={arXiv preprint arXiv:1806.03822}, year={2018} }*\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96bb8fb1-84e1-43d3-981b-769856e1c204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train-v2.0.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# We will use the train split of SQuAD2.0\n",
    "original_data_file = \"train-v2.0.json\"\n",
    "\n",
    "# The data was mirrored in the following bucket\n",
    "original_data_location = (\n",
    "    f\"s3://sagemaker-example-files-prod-{aws_region}/datasets/text/squad2.0/{original_data_file}\"\n",
    ")\n",
    "S3Downloader.download(original_data_location, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c3efd-63ae-4d01-9b13-cd9c48c1af92",
   "metadata": {},
   "source": [
    "The Text2Text generation model can be fine-tuned on any text data provided that the data is in the expected format. The data must include a training and an optional validation parts. The best model is selected according to the validation loss, calculated at the end of each epoch. If a validation set is not given, an (adjustable) percentage of the training data is automatically split and used for validation.\n",
    "\n",
    "The training data must be formatted in JSON lines (`.jsonl`) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training folder can also contain a `template.json` file describing the input and output formats.\n",
    "\n",
    "If no template file is given, the following default template will be used:\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"{prompt}\",\n",
    "    \"completion\": \"{completion}\"\n",
    "}\n",
    "```\n",
    "In this case, the data in the JSON lines entries must include `prompt` and `completion` fields.\n",
    "\n",
    "In this demo, we are going to use a custom template (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3002bc68-2779-4fc0-a68b-cbdd0b3663bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "local_data_file = \"task-data.jsonl\"  # any name with .jsonl extension\n",
    "\n",
    "with open(original_data_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(local_data_file, \"w\") as f:\n",
    "    for article in data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            # iterate over questions for a given paragraph\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                if qas[\"is_impossible\"]:\n",
    "                    # the question is relevant, but cannot be answered\n",
    "                    example = {\"context\": paragraph[\"context\"], \"question\": qas[\"question\"]}\n",
    "                    json.dump(example, f)\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\",\n",
    "    \"completion\": \"{question}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc435de-a9c3-403e-a24a-f5ef032755cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtraining data:\u001b[0m s3://sagemaker-us-east-1-327265443405/train_data\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "train_data_location = f\"s3://{output_bucket}/train_data\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"{bold}training data:{unbold} {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c448df-769f-41de-ba3b-e9ed0dccb041",
   "metadata": {},
   "source": [
    "#### 2.2. Preparing for training\n",
    "\n",
    "We need to collect several pieces of information, including pre-trained model artifact URI and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e35da74-e166-41e8-b7d3-3804911b522e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mimage uri:\u001b[0m 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      "\u001b[1mmodel uri:\u001b[0m s3://jumpstart-cache-prod-us-east-1/huggingface-training/train-huggingface-text2text-flan-t5-small.tar.gz\n",
      "\u001b[1mscript uri:\u001b[0m s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.1.2/sourcedir.tar.gz\n",
      "\u001b[1moutput location:\u001b[0m s3://sagemaker-us-east-1-327265443405/demo-fine-tune-flan-t5/\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "# Training instance will use this image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Pre-trained model\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "# Script to execute on the training instance\n",
    "train_script_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "output_location = f\"s3://{output_bucket}/demo-fine-tune-flan-t5/\"\n",
    "\n",
    "print(f\"{bold}image uri:{unbold} {train_image_uri}\")\n",
    "print(f\"{bold}model uri:{unbold} {train_model_uri}\")\n",
    "print(f\"{bold}script uri:{unbold} {train_script_uri}\")\n",
    "print(f\"{bold}output location:{unbold} {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0df7c-0414-49d9-953d-1d48675236c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# We will override some default hyperparameters with custom values\n",
    "hyperparameters[\"epochs\"] = \"1\"\n",
    "print(hyperparameters)\n",
    "\n",
    "# Note that the maximum output length is set to 128 tokens by default.\n",
    "# The targets in your data (i.e., ground truth responses) will be truncated to this size.\n",
    "# You can override this behavior, e.g.,\n",
    "# hyperparameters[\"max_output_length\"] = \"256\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e4bd5-e013-400d-82df-4056a98dbee7",
   "metadata": {},
   "source": [
    "#### 2.3. Parameter-efficient fine-tuning\n",
    "\n",
    "As models get larger, fine-tuning such models becomes increasingly expensive. [Parameter-efficient fine-tuning](https://github.com/huggingface/peft) (PEFT) approaches reduce training cost by not updating (\"freezing\") most of the parameters. Only a small fraction of parameters ends up being tuned. Some PEFT approaches extend the original model and only allow the additional parameters to be tuned. Importantly, it has been observed that PEFT can often result in model performance comparable to full tine-tuning.\n",
    "\n",
    "For large models, PEFT approaches considerably reduce both training time and memory footprint compared to full fine-tuning. For example, fine-tuning a large `xxl` model on a `g5.*` instance is only possible with PEFT.\n",
    "\n",
    "Use of PEFT can be controlled through `hyperparameters[\"peft_type\"]`. Lora is currently the only supported PEFT approach. Set the parameter value to `\"lora\"` to enable Lora, or use `\"none\"` for full fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf3377-4cd7-40df-876b-e0fa2f57dc8f",
   "metadata": {},
   "source": [
    "#### 2.4. Starting training\n",
    "\n",
    "We are now ready to start the training job. This can take a while to complete, from 20 minutes to several hours, depending on the model size, amount of data, and so on (e.g., it can take a few hours for the `xl` model, 40k examples and 3 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068d6c5a-00a1-48d0-a2cd-86bcdd15cb0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: js-demo-flan-t5-xl-3-2023-08-16-06-43-52-749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mjob name:\u001b[0m js-demo-flan-t5-xl-3-2023-08-16-06-43-52-749\n",
      "2023-08-16 06:43:52 Starting - Starting the training job.........\n",
      "2023-08-16 06:45:02 Starting - Preparing the instances for training......\n",
      "2023-08-16 06:46:13 Downloading - Downloading input data....................................\n",
      "2023-08-16 06:52:10 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:12,000 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:12,035 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:12,046 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:12,048 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:13,534 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.19.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.9.2.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.4-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.0.2-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.9.2->-r requirements.txt (line 3)) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft==0.3.0->-r requirements.txt (line 4)) (4.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 2)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.3.0->-r requirements.txt (line 4)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft==0.3.0->-r requirements.txt (line 4)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.12.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811220 sha256=d685921f03763c1fcef5da611fa2e000e82d3b49bb42e12520c09d9c8adeaaf0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/31/4e/6c/adcb68c67e187eb0fb3939cc0ed6357effa4660dbd006f5fa4\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, deepspeed, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.16.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.16.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.16.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.9.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.9.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.9.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.19.0 datasets-2.12.0 deepspeed-0.9.2 peft-0.3.0 sagemaker-jumpstart-huggingface-script-utilities-1.0.2 sagemaker-jumpstart-script-utilities-1.1.4 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,263 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,263 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,325 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,375 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,424 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:26,436 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"batch_size\": \"64\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epochs\": \"3\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"500\",\n",
      "        \"evalaution_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": \"1\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"500\",\n",
      "        \"logging_strategy\": \"steps\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_eval_samples\": \"-1\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_output_length\": \"128\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"pad_to_max_length\": \"True\",\n",
      "        \"peft_type\": \"none\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"save_total_limit\": \"2\",\n",
      "        \"seed\": \"42\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.05\",\n",
      "        \"warmup_ratio\": \"0.0\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"js-demo-flan-t5-xl-3-2023-08-16-06-43-52-749\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.1.2/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epochs\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"peft_type\":\"none\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.1.2/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"batch_size\":\"64\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epochs\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"500\",\"evalaution_strategy\":\"epoch\",\"gradient_accumulation_steps\":\"1\",\"gradient_checkpointing\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"0.0001\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"500\",\"logging_strategy\":\"steps\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_eval_samples\":\"-1\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_output_length\":\"128\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"pad_to_max_length\":\"True\",\"peft_type\":\"none\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"epoch\",\"save_total_limit\":\"2\",\"seed\":\"42\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.05\",\"warmup_ratio\":\"0.0\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"js-demo-flan-t5-xl-3-2023-08-16-06-43-52-749\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/text2text/prepack/v1.1.2/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"False\",\"--batch_size\",\"64\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epochs\",\"3\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"500\",\"--evalaution_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"1\",\"--gradient_checkpointing\",\"True\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"0.0001\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"500\",\"--logging_strategy\",\"steps\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_eval_samples\",\"-1\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"-1\",\"--max_output_length\",\"128\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--pad_to_max_length\",\"True\",\"--peft_type\",\"none\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"epoch\",\"--save_total_limit\",\"2\",\"--seed\",\"42\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.05\",\"--warmup_ratio\",\"0.0\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_EVALAUTION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_EVAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_OUTPUT_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=True\u001b[0m\n",
      "\u001b[34mSM_HP_PEFT_TYPE=none\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=2\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epochs 3 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_eval_samples -1 --max_grad_norm 1.0 --max_input_length -1 --max_output_length 128 --max_steps -1 --max_train_samples -1 --pad_to_max_length True --peft_type none --preprocessing_num_workers None --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --train_data_split_seed 0 --validation_split_ratio 0.05 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:52:30.705: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:30,709 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-16 06:52:30,728 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mExtracting model to /tmp/extracted\u001b[0m\n",
      "\u001b[34mINFO:__main__:Extracting model to /tmp/extracted\u001b[0m\n",
      "\u001b[34mFound existing /opt/ml/model. This directory will be emptied.\u001b[0m\n",
      "\u001b[34mWARNING:__main__:Found existing /opt/ml/model. This directory will be emptied.\u001b[0m\n",
      "\u001b[34mError deleting /opt/ml/model: (<class 'OSError'>, OSError(16, 'Device or resource busy'), <traceback object at 0x7fc303b16140>)\u001b[0m\n",
      "\u001b[34mWARNING:__main__:Error deleting /opt/ml/model: (<class 'OSError'>, OSError(16, 'Device or resource busy'), <traceback object at 0x7fc303b16140>)\u001b[0m\n",
      "\u001b[34mINFO:root:Preparing training data...\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e8893f91f59fd34b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1961.79it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e8893f91f59fd34b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mNo validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34mINFO:data_preprocessor:No validation data provided, splitting automatically.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/41323 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 1000/41323 [00:00<00:31, 1267.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 2000/41323 [00:01<00:30, 1290.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 3000/41323 [00:02<00:30, 1275.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 4000/41323 [00:03<00:29, 1277.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 5000/41323 [00:03<00:28, 1285.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 6000/41323 [00:04<00:27, 1286.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 7000/41323 [00:05<00:27, 1237.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 8000/41323 [00:06<00:26, 1251.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 9000/41323 [00:07<00:25, 1261.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 10000/41323 [00:07<00:24, 1273.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 11000/41323 [00:08<00:23, 1277.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 12000/41323 [00:09<00:22, 1278.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███▏      | 13000/41323 [00:10<00:22, 1282.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 14000/41323 [00:10<00:21, 1287.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▋      | 15000/41323 [00:11<00:20, 1284.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 16000/41323 [00:12<00:19, 1286.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 17000/41323 [00:13<00:18, 1281.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▎     | 18000/41323 [00:14<00:18, 1278.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 19000/41323 [00:14<00:18, 1237.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 20000/41323 [00:15<00:17, 1247.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 21000/41323 [00:16<00:16, 1256.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 22000/41323 [00:17<00:15, 1266.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 23000/41323 [00:18<00:14, 1264.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 24000/41323 [00:18<00:13, 1266.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 25000/41323 [00:19<00:12, 1263.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 26000/41323 [00:20<00:12, 1269.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 27000/41323 [00:21<00:11, 1265.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 28000/41323 [00:22<00:10, 1273.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 29000/41323 [00:22<00:09, 1272.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 30000/41323 [00:23<00:08, 1275.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 31000/41323 [00:24<00:08, 1240.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 32000/41323 [00:25<00:07, 1251.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 33000/41323 [00:26<00:06, 1256.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 34000/41323 [00:26<00:05, 1265.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▍ | 35000/41323 [00:27<00:04, 1270.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 36000/41323 [00:28<00:04, 1279.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 37000/41323 [00:29<00:03, 1282.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 38000/41323 [00:29<00:02, 1280.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 39000/41323 [00:30<00:01, 1277.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 40000/41323 [00:31<00:01, 1277.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 41000/41323 [00:32<00:00, 1279.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 41323/41323 [00:32<00:00, 1272.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2175 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 1000/2175 [00:00<00:01, 1162.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 2000/2175 [00:01<00:00, 1223.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2175/2175 [00:01<00:00, 1223.09 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/41323 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 41323/41323 [00:00<00:00, 442536.59 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/2175 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 2175/2175 [00:00<00:00, 297153.46 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking DeepSpeed with command ['deepspeed', '--num_gpus', '4', 'train.py', '--model_dir', '/opt/ml/model', '--pretrained_model_dir', '/tmp/extracted', '--generation_max_length', '128', '--peft_type', 'none', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--adam_epsilon', '1e-08', '--auto_find_batch_size', 'False', '--batch_size', '64', '--dataloader_drop_last', 'False', '--dataloader_num_workers', '0', '--early_stopping_patience', '3', '--early_stopping_threshold', '0.0', '--epochs', '3', '--eval_accumulation_steps', 'None', '--eval_steps', '500', '--evalaution_strategy', 'epoch', '--gradient_accumulation_steps', '1', '--gradient_checkpointing', 'True', '--label_smoothing_factor', '0', '--learning_rate', '0.0001', '--load_best_model_at_end', 'True', '--logging_first_step', 'False', '--logging_nan_inf_filter', 'True', '--logging_steps', '500', '--logging_strategy', 'steps', '--lr_scheduler_type', 'constant_with_warmup', '--max_grad_norm', '1.0', '--max_steps', '-1', '--save_steps', '500', '--save_strategy', 'epoch', '--save_total_limit', '2', '--seed', '42', '--warmup_ratio', '0.0', '--warmup_steps', '0', '--weight_decay', '0.0']\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:25,473] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:25,521] [INFO] [runner.py:541:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --model_dir /opt/ml/model --pretrained_model_dir /tmp/extracted --generation_max_length 128 --peft_type none --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --batch_size 64 --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epochs 3 --eval_accumulation_steps None --eval_steps 500 --evalaution_strategy epoch --gradient_accumulation_steps 1 --gradient_checkpointing True --label_smoothing_factor 0 --learning_rate 0.0001 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 500 --logging_strategy steps --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_steps -1 --save_steps 500 --save_strategy epoch --save_total_limit 2 --seed 42 --warmup_ratio 0.0 --warmup_steps 0 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:222:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:222:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:222:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:222:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,990] [INFO] [launch.py:247:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:54:27,991] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(pretrained_model_dir='/tmp/extracted', output_data_dir='/opt/ml/output/data', checkpoint_dir='/tmp', model_dir='/opt/ml/model', epochs=3, max_steps=-1, generation_max_length=128, generation_num_beams=4, learning_rate=0.0001, batch_size=64, seed=42, bf16='auto', offload='auto', num_gpus=4, gradient_checkpointing='True', load_best_model_at_end='True', early_stopping_patience=3, early_stopping_threshold=0.0, gradient_accumulation_steps=1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_steps=500, logging_nan_inf_filter='True', save_strategy='epoch', save_steps=500, save_total_limit='2', dataloader_drop_last='False', dataloader_num_workers=0, evaluation_strategy='epoch', eval_steps=500, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_ratio=0.0, warmup_steps=0, peft_type='none', local_rank=1).\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(pretrained_model_dir='/tmp/extracted', output_data_dir='/opt/ml/output/data', checkpoint_dir='/tmp', model_dir='/opt/ml/model', epochs=3, max_steps=-1, generation_max_length=128, generation_num_beams=4, learning_rate=0.0001, batch_size=64, seed=42, bf16='auto', offload='auto', num_gpus=4, gradient_checkpointing='True', load_best_model_at_end='True', early_stopping_patience=3, early_stopping_threshold=0.0, gradient_accumulation_steps=1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_steps=500, logging_nan_inf_filter='True', save_strategy='epoch', save_steps=500, save_total_limit='2', dataloader_drop_last='False', dataloader_num_workers=0, evaluation_strategy='epoch', eval_steps=500, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_ratio=0.0, warmup_steps=0, peft_type='none', local_rank=3).\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(pretrained_model_dir='/tmp/extracted', output_data_dir='/opt/ml/output/data', checkpoint_dir='/tmp', model_dir='/opt/ml/model', epochs=3, max_steps=-1, generation_max_length=128, generation_num_beams=4, learning_rate=0.0001, batch_size=64, seed=42, bf16='auto', offload='auto', num_gpus=4, gradient_checkpointing='True', load_best_model_at_end='True', early_stopping_patience=3, early_stopping_threshold=0.0, gradient_accumulation_steps=1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_steps=500, logging_nan_inf_filter='True', save_strategy='epoch', save_steps=500, save_total_limit='2', dataloader_drop_last='False', dataloader_num_workers=0, evaluation_strategy='epoch', eval_steps=500, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_ratio=0.0, warmup_steps=0, peft_type='none', local_rank=0).\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(pretrained_model_dir='/tmp/extracted', output_data_dir='/opt/ml/output/data', checkpoint_dir='/tmp', model_dir='/opt/ml/model', epochs=3, max_steps=-1, generation_max_length=128, generation_num_beams=4, learning_rate=0.0001, batch_size=64, seed=42, bf16='auto', offload='auto', num_gpus=4, gradient_checkpointing='True', load_best_model_at_end='True', early_stopping_patience=3, early_stopping_threshold=0.0, gradient_accumulation_steps=1, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_steps=500, logging_nan_inf_filter='True', save_strategy='epoch', save_steps=500, save_total_limit='2', dataloader_drop_last='False', dataloader_num_workers=0, evaluation_strategy='epoch', eval_steps=500, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_ratio=0.0, warmup_steps=0, peft_type='none', local_rank=2).\u001b[0m\n",
      "\u001b[34mWARNING:root:Training script received unknown args: ['--evalaution_strategy', 'epoch'].\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Model class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\u001b[0m\n",
      "\u001b[34mINFO:root:DeepSpeed config: DeepSpeedConfig(bf16=False, offload=True, zero_opt_overwrites=None, global_overwrites=None, ds_lr_scheduler='WarmupLR').\u001b[0m\n",
      "\u001b[34mINFO:root:Collecting training args...\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Model class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\u001b[0m\n",
      "\u001b[34mINFO:root:DeepSpeed config: DeepSpeedConfig(bf16=False, offload=True, zero_opt_overwrites=None, global_overwrites=None, ds_lr_scheduler='WarmupLR').\u001b[0m\n",
      "\u001b[34mINFO:root:Collecting training args...\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Model class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\u001b[0m\n",
      "\u001b[34mINFO:root:DeepSpeed config: DeepSpeedConfig(bf16=False, offload=True, zero_opt_overwrites=None, global_overwrites=None, ds_lr_scheduler='WarmupLR').\u001b[0m\n",
      "\u001b[34mINFO:root:Collecting training args...\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.37s/it]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:02,664] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\u001b[0m\n",
      "\u001b[34mINFO:root:Model class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\u001b[0m\n",
      "\u001b[34mINFO:root:DeepSpeed config: DeepSpeedConfig(bf16=False, offload=True, zero_opt_overwrites=None, global_overwrites=None, ds_lr_scheduler='WarmupLR').\u001b[0m\n",
      "\u001b[34mINFO:root:Collecting training args...\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:root:Training...\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:03,670] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mINFO:root:Training...\u001b[0m\n",
      "\u001b[34mINFO:root:Training...\u001b[0m\n",
      "\u001b[34mINFO:root:Training...\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mINFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\u001b[0m\n",
      "\u001b[34malgo-1:664:1546 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:664:1546 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:667:1547 [3] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:667:1547 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:665:1548 [1] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:666:1549 [2] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:665:1548 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:666:1549 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:06,694] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.945326566696167 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.913283586502075 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.911389350891113 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 28.911581993103027 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,550] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,605] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,605] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,605] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,605] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,703] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,704] [INFO] [utils.py:786:see_memory_usage] MA 10.62 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,704] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.62 GB, percent = 5.3%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,709] [INFO] [stage3.py:113:__init__] Reduce bucket size 4194304\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:38,709] [INFO] [stage3.py:114:__init__] Prefetch bucket size 3774873\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.598823308944702 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.622101783752441 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.621261596679688 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.621285676956177 seconds\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:54,432] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:54,433] [INFO] [utils.py:786:see_memory_usage] MA 10.62 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:55:54,433] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.98 GB, percent = 5.3%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 251904 in 124 params\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,389] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,390] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 10.62 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,390] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 34.05 GB, percent = 9.1%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,490] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,491] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:03,491] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 34.07 GB, percent = 9.1%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:13,912] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:13,913] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:13,913] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.6 GB, percent = 15.7%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,013] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,014] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,014] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.6 GB, percent = 15.7%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,211] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,212] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,212] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.26 GB, percent = 16.4%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,414] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,415] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:14,415] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 67.96 GB, percent = 18.2%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:18,502] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:18,502] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 10.62 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:18,503] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 98.85 GB, percent = 26.5%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:18,503] [INFO] [stage3.py:392:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00035381317138671875 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00037217140197753906 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00040411949157714844 seconds\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,755] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [utils.py:786:see_memory_usage] MA 0.02 GB         Max_MA 0.51 GB         CA 10.87 GB         Max_CA 11 GB\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 115.19 GB, percent = 30.8%\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f3a5be7bfa0>\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,758] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,758] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,758] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,758] [INFO] [config.py:959:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,758] [INFO] [config.py:959:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3422ea43a0>\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,759] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  16\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=4194304 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=3774873 param_persistence_threshold=20480 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,760] [INFO] [config.py:959:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23,761] [INFO] [config.py:945:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.0001, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 4.194304e+06, \n",
      "        \"stage3_prefetch_bucket_size\": 3.774874e+06, \n",
      "        \"stage3_param_persistence_threshold\": 2.048000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 16, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0002982616424560547 seconds\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 41323\u001b[0m\n",
      "\u001b[34mNum examples = 41323\n",
      "  Num Epochs = 3\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mNum Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 1938\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 1938\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34m0%|          | 0/1938 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.789: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.789: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.790: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.815 algo-1:665 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.815 algo-1:667 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.816 algo-1:666 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.844 algo-1:667 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.844 algo-1:665 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.844 algo-1:666 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:665 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:666 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:667 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:665 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:666 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:667 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:665 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:666 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:667 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:665 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:666 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.845 algo-1:667 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.855: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.882 algo-1:664 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.909 algo-1:664 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.909 algo-1:664 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.910 algo-1:664 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.910 algo-1:664 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:23.910 algo-1:664 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/1938 [00:17<9:15:39, 17.21s/it]\u001b[0m\n",
      "\u001b[34m[2023-08-16 06:56:57,979] [WARNING] [stage3.py:1826:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 2/1938 [00:34<9:11:22, 17.09s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1938 [00:48<8:27:44, 15.74s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1938 [01:02<8:07:25, 15.12s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1938 [01:16<7:56:11, 14.78s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/1938 [01:30<7:49:19, 14.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/1938 [01:45<7:44:58, 14.45s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/1938 [01:59<7:41:51, 14.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/1938 [02:13<7:39:53, 14.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1938 [02:27<7:38:38, 14.27s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 11/1938 [02:41<7:37:42, 14.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/1938 [02:56<7:36:46, 14.23s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 13/1938 [03:10<7:36:17, 14.22s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 14/1938 [03:24<7:35:59, 14.22s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 15/1938 [03:38<7:35:30, 14.21s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 16/1938 [03:52<7:34:56, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 17/1938 [04:06<7:34:35, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 18/1938 [04:21<7:34:16, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 19/1938 [04:35<7:34:04, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 20/1938 [04:49<7:34:05, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 21/1938 [05:03<7:33:29, 14.19s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 22/1938 [05:17<7:33:18, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 23/1938 [05:32<7:32:58, 14.19s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 24/1938 [05:46<7:32:58, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 25/1938 [06:00<7:32:32, 14.19s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 26/1938 [06:14<7:32:21, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 27/1938 [06:28<7:32:07, 14.20s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 28/1938 [06:43<7:31:55, 14.20s/it]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m\n\u001b[1;32m     15\u001b[0m sm_estimator \u001b[38;5;241m=\u001b[39m Estimator(\n\u001b[1;32m     16\u001b[0m     role\u001b[38;5;241m=\u001b[39maws_role,\n\u001b[1;32m     17\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39mtrain_image_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     metric_definitions\u001b[38;5;241m=\u001b[39mtraining_metric_definitions,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Launch a SageMaker training job over data located in the given S3 path.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# For larger models it is recommended to set wait=False and monitor job\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# status through SageMaker console because training can take hours\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43msm_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_location\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_job_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:300\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1252\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2398\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4786\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4766\u001b[0m     \u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4767\u001b[0m \n\u001b[1;32m   4768\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4786\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6605\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   6603\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 6605\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   6608\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = \"-\".join(model_id.split(\"-\")[2:])  # get the most informative part of ID\n",
    "training_job_name = name_from_base(f\"js-demo-{model_name}-{hyperparameters['epochs']}\")\n",
    "print(f\"{bold}job name:{unbold} {training_job_name}\")\n",
    "\n",
    "training_metric_definitions = [\n",
    "    {\"Name\": \"val_loss\", \"Regex\": \"'eval_loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"'loss': ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"'epoch': ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "sm_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    source_dir=train_script_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    volume_size=300,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_location,\n",
    "    metric_definitions=training_metric_definitions,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker training job over data located in the given S3 path.\n",
    "# For larger models it is recommended to set wait=False and monitor job\n",
    "# status through SageMaker console because training can take hours\n",
    "sm_estimator.fit({\"training\": train_data_location}, job_name=training_job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524dead-769d-4b5d-88b2-e9587dab474e",
   "metadata": {},
   "source": [
    "Performance metrics such as training and validation loss can be accessed through CloudWatch during training. We can also fetch the most recent snapshot of metrics as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683497e-dc98-4fe3-ac15-d09cd58817e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# If you specified wait=False above, you need to wait for a couple of\n",
    "# minutes for the job to start before running this cell.\n",
    "# This cell can be executed while the job is still in progress\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ba317-699a-4b86-9cf4-254a3f5bdbe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Deploying inference endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629eaa2c-b415-4bd7-b8bc-500f118862e5",
   "metadata": {},
   "source": [
    "Remainder of the notebook should be executed once the training job is successfully completed. Recall that variable `training_job_name` contains job name and `output_location` points to an S3 location with a fine-tuned model artifact.\n",
    "\n",
    "We will create two inference endpoints, one for the original pre-trained model, and one for the fine-tuned model. We will then run the same request against the two endpoints and compare the results.\n",
    "\n",
    "Note that each endpoint deployment can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e490b-2c5e-493d-a3f8-03345742f994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Retrieve the inference docker image URI. This is the base HuggingFace container image\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=inference_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a9d26-11fa-4788-a768-2535060b0aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import model_uris, script_uris\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Retrieve the URI of the pre-trained model\n",
    "pre_trained_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "pre_trained_name = name_from_base(f\"jumpstart-demo-pre-trained-{model_id}\")\n",
    "\n",
    "# Create a model instance for the pre-trained model. Note that we need to pass\n",
    "# Predictor class to be able to run inference through the SageMaker API\n",
    "pre_trained_model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=pre_trained_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=pre_trained_name,\n",
    ")\n",
    "\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {pre_trained_model_uri}\")\n",
    "print(\"Deploying an endpoint ...\")\n",
    "\n",
    "# Deploy the pre-trained model\n",
    "pre_trained_predictor = pre_trained_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=pre_trained_name,\n",
    ")\n",
    "print(f\"{newline}Deployed an endpoint {pre_trained_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cab2be-f433-41c3-bee5-e8e0d61a7a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "fine_tuned_name = name_from_base(f\"jumpstart-demo-fine-tuned-{model_id}\")\n",
    "fine_tuned_model_uri = f\"{output_location}{training_job_name}/output/model.tar.gz\"\n",
    "\n",
    "# Create a model instance for the fine-tuned model\n",
    "fine_tuned_model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=fine_tuned_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=fine_tuned_name,\n",
    ")\n",
    "\n",
    "print(f\"{bold}image URI:{unbold}{newline} {deploy_image_uri}\")\n",
    "print(f\"{bold}model URI:{unbold}{newline} {fine_tuned_model_uri}\")\n",
    "print(\"Deploying an endpoint ...\")\n",
    "\n",
    "# Deploy the fine-tuned model\n",
    "fine_tuned_predictor = fine_tuned_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=fine_tuned_name,\n",
    ")\n",
    "print(f\"{newline}Deployed an endpoint {fine_tuned_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58114831-c08e-4542-bc5e-852826785356",
   "metadata": {},
   "source": [
    "### 4. Running inference queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ebf33c-dc6f-41a9-b137-1e2ffaa8de2e",
   "metadata": {},
   "source": [
    "As the name suggests, a Text2Text model such as FLAN T5 receives a piece of text as input, and generates text as output. The input text will contain the description of the task. In this demo, our task is to generate questions given a piece of text. The questions must be relevant to the text, but the text should contain no answer. Such a task could arise when automating gathering additional information, or identifying gaps in technical documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598072e-fc52-40c0-8343-1a74e35cfa3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\"\n",
    "\n",
    "# Sources: Wikipedia, AWS Documentation\n",
    "test_paragraphs = [\n",
    "    \"\"\"\n",
    "Adelaide is the capital city of South Australia, the state's largest city and the fifth-most populous city in Australia. \"Adelaide\" may refer to either Greater Adelaide (including the Adelaide Hills) or the Adelaide city centre. The demonym Adelaidean is used to denote the city and the residents of Adelaide. The Traditional Owners of the Adelaide region are the Kaurna people. The area of the city centre and surrounding parklands is called Tarndanya in the Kaurna language.\n",
    "Adelaide is situated on the Adelaide Plains north of the Fleurieu Peninsula, between the Gulf St Vincent in the west and the Mount Lofty Ranges in the east. Its metropolitan area extends 20 km (12 mi) from the coast to the foothills of the Mount Lofty Ranges, and stretches 96 km (60 mi) from Gawler in the north to Sellicks Beach in the south.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance. You can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). You can dynamically change the configuration of a volume attached to an instance.\n",
    "We recommend Amazon EBS for data that must be quickly accessible and requires long-term persistence. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. Amazon EBS is well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \n",
    "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. \n",
    "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f1f73-4ba9-422a-adf1-d94a686c2b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Parameters of (output) text generation. A great introduction to generation\n",
    "# parameters can be found at https://huggingface.co/blog/how-to-generate\n",
    "parameters = {\n",
    "    \"max_length\": 40,  # restrict the length of the generated text\n",
    "    \"num_return_sequences\": 5,  # we will inspect several model outputs\n",
    "    \"num_beams\": 10,  # use beam search\n",
    "}\n",
    "\n",
    "\n",
    "# Helper functions for running inference queries\n",
    "def query_endpoint_with_json_payload(payload, endpoint_name):\n",
    "    encoded_json = json.dumps(payload).encode(\"utf-8\")\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def generate_questions(endpoint_name, text):\n",
    "    expanded_prompt = prompt.replace(\"{context}\", text)\n",
    "    payload = {\"text_inputs\": expanded_prompt, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(payload, endpoint_name=endpoint_name)\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    for i, generated_text in enumerate(generated_texts):\n",
    "        print(f\"Response {i}: {generated_text}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d04ea2-82a0-446a-b64d-ab68442081a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{bold}Prompt:{unbold} {repr(prompt)}\")\n",
    "for paragraph in test_paragraphs:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{bold}pre-trained{unbold}\")\n",
    "    generate_questions(pre_trained_name, paragraph)\n",
    "    print(f\"{bold}fine-tuned{unbold}\")\n",
    "    generate_questions(fine_tuned_name, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13af34-3364-4ca0-ad89-929996642a80",
   "metadata": {},
   "source": [
    "The pre-trained model was not specifically trained to generate unanswerable questions. Despite the input prompt, it tends to generate questions that can be answered from the text. The fine-tuned model is generally better at this task, and the improvement is more prominent for larger models (e.g., xl rather than base)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beafe5b-07d9-4095-9f4e-7f75b4104c95",
   "metadata": {},
   "source": [
    "### 5. Cleaning up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99076e38-16db-4b77-9545-d21796af4ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "pre_trained_predictor.delete_model()\n",
    "pre_trained_predictor.delete_endpoint()\n",
    "fine_tuned_predictor.delete_model()\n",
    "fine_tuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2c5d5",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text2text-fine-tuning-flan-t5.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
